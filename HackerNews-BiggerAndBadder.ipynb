{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "01bd76ab9782c73646930a8c6ac495eb93d52786"
   },
   "source": [
    "# Hacker News Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667e28a0ca914e23adfddb15216b89a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TqdmDeprecationWarning: Please use `tqdm.pandas(...)` instead of `tqdm_pandas(tqdm(...))`.\n"
     ]
    }
   ],
   "source": [
    "#!pip install textblob\n",
    "from textblob import TextBlob\n",
    "import bokeh\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import html \n",
    "import dask\n",
    "import re\n",
    "import dask.dataframe as dd\n",
    "from tqdm import tqdm_pandas\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import json\n",
    "\n",
    "def save_df(df):\n",
    "    df.to_feather('data/df_save.feather')\n",
    "    print('Dataframe Saved')\n",
    "    \n",
    "def load_df():\n",
    "    pd.read_feather('data/df_save.feather')\n",
    "    print('Dataframe Loaded')\n",
    "    \n",
    "tqdm_pandas(tqdm())\n",
    "\n",
    "#!pip install line_profiler\n",
    "# Load the profiler into your Jupyter notebook\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This time I ran the query in Google BigQuery and copied the files to Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dask kept giving me errors. \n",
    "d0 = pd.read_csv('data/hn_main_query/hacker_news_full_comments0.csv',engine='python')\n",
    "print(\"dataframe1\")\n",
    "d1 = pd.read_csv('data/hn_main_query/hacker_news_full_comments1.csv',engine='python')\n",
    "print(\"dataframe2\")\n",
    "d2 = pd.read_csv('data/hn_main_query/hacker_news_full_comments2.csv',engine='python')\n",
    "print(\"dataframe3\")\n",
    "d3 = pd.read_csv('data/hn_main_query/hacker_news_full_comments3.csv',engine='python')\n",
    "print(\"dataframe4\")\n",
    "d4 = pd.read_csv('data/hn_main_query/hacker_news_full_comments4.csv',engine='python')\n",
    "print(\"dataframe5\")\n",
    "d5 = pd.read_csv('data/hn_main_query/hacker_news_full_comments5.csv',engine='python')\n",
    "print(\"dataframe6\")\n",
    "d6 = pd.read_csv('data/hn_main_query/hacker_news_full_comments6.csv',engine='python')\n",
    "print(\"dataframe7\")\n",
    "d7 = pd.read_csv('data/hn_main_query/hacker_news_full_comments7.csv',engine='python')\n",
    "print(\"dataframe8\")\n",
    "d8 = pd.read_csv('data/hn_main_query/hacker_news_full_comments8.csv',engine='python')\n",
    "print(\"dataframe9\")\n",
    "d9 = pd.read_csv('data/hn_main_query/hacker_news_full_comments9.csv',engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([d0, d1, d2, d3, d4, d5, d6, d7, d8, d9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect shape of completed dask csv import. Verify that all rows are present. (15,825,859) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b88e551ef1d6c2562e1beff9d60be3c88e83ce02"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a6d7b7611d8aa093224b66ec3131a8586cc71b20"
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save query results to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.to_csv('data/hn_commentors_all_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV back into new Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds2 = pd.read_csv('data/hn_commentors_all_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect and verify that all rows are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds2.shape)\n",
    "display(ds2.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all  `author` and `text` NaN rows from Dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = ds2.text.isna().sum()\n",
    "print('This many nans:', nans)\n",
    "ds2 = ds2.dropna(subset=['commentor', 'text'])\n",
    "print('New Shape after nan removal:', ds2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = ds2.parent_type.isna().sum()\n",
    "print('This many parent_type nans:', nans)\n",
    "nans = ds2.story_title.isna().sum()\n",
    "print('This many story_title nans:', nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove `Unnamed: 0` Column and fill in empty Titles for replies on comment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds2 = ds2.loc[:, ~ds2.columns.str.match('Unnamed')]\n",
    "ds2['story_title'] = ds2.story_title.fillna('Another Comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = ds2.story_title.isna().sum()\n",
    "print('This many story_title nans:', nans)\n",
    "display(ds2.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis and Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "800fd0be97d8d4111ebcb7c6d5968cc944bc08b7"
   },
   "outputs": [],
   "source": [
    "def encode_decode(text):\n",
    "    \"\"\"\n",
    "    Utility function to clean text by decoding HTML text.\n",
    "    \"\"\"\n",
    "    unescaped = html.unescape(text)\n",
    "    return unescaped\n",
    "\n",
    "def noHTML(text):\n",
    "    \"\"\"\n",
    "    Utility function to clean text by removing HTML flags.\n",
    "    \"\"\"\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', text)\n",
    "    return cleantext\n",
    "\n",
    "def noURLS(text):\n",
    "    \"\"\"\n",
    "    Utility function to clean text by removing links\n",
    "    using simple regex statements.\n",
    "    \"\"\"\n",
    "    return ''.join(re.sub(r\"http\\S+\", \"\", text))\n",
    "\n",
    "\n",
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Utility function to classify sentiment of passed text\n",
    "    using textblob's sentiment method. Return the polarity\n",
    "    score as a float within the range [-1.0, 1.0]\n",
    "    \n",
    "    The polarity score is a float within the range [-1.0, 1.0] \n",
    "    where negative value indicates negative text \n",
    "    and positive value indicates that the given \n",
    "    text is positive.\n",
    "\n",
    "    The subjectivity is a float within the range [0.0, 1.0] \n",
    "    where 0.0 is very objective and 1.0 is very subjective.\n",
    "    \"\"\"\n",
    "    analysis = TextBlob(text).sentiment\n",
    "    polarity = analysis.polarity\n",
    "    subjectivity = analysis.subjectivity\n",
    "    return polarity, subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply text cleaning to comment texts and create new column in Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_pandas(tqdm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2['cleaned_comment'] = ds2.text.progress_apply(lambda x: noURLS(noHTML(encode_decode(x))))\n",
    "ds2['cleaned_title'] = ds2.story_title.progress_apply(lambda x: noURLS(noHTML(encode_decode(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply sentiment analysis (TextBlob.polarity) to each cleaned Comment text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2['comment_sentiment'] = ds2.cleaned_comment.progress_apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3 = ds2.loc[:, ~ds2.columns.str.match('Unnamed')]\n",
    "ds3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds3.to_csv('data/hn_all_w_sentiment_cleaned.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds3.shape)\n",
    "#display(ds3.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsx = ds3.drop(columns=['text', 'story_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = pd.DataFrame(dsx['comment_sentiment'].tolist(), index=dsx.index)\n",
    "sdf.columns = ['polarity', 'subjectivity']\n",
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsx[\"comment_polarity\"] = sdf.polarity\n",
    "dsx[\"comment_subjectivity\"] = sdf.subjectivity\n",
    "dsx = dsx.drop(columns=['comment_sentiment'])\n",
    "dsx.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dsx.to_csv('data/hn_all_w_sentiment_cleaned_inplace.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleaned / analyzed data back into dataframe from CSV - X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15397309, 19)\n",
      "CPU times: user 1min 41s, sys: 12.2 s, total: 1min 53s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# IMPORT FROM CSV's\n",
    "ds4 = pd.read_csv('data/hn_all_w_sentiment_cleaned_inplace.csv')\n",
    "print(ds4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds4 = ds4.loc[:, ~ds4.columns.str.match('Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This many nans: 15397309\n"
     ]
    }
   ],
   "source": [
    "nans = ds4.ranking.isna().sum()\n",
    "print('This many nans:', nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oops, looks like ranking wasn't actually on that BigQuery table even though the field is there. I'll need to pull it in and merge it here by commentid from the comments table. \n",
    "\n",
    "After a bit of investigation I found that the table `bigquery-public-data.hacker_news.full_201510` does contain comment ranking type entries, but the `bigquery-public-data.hacker_news.full` (the one that is continuously updated) does not. \n",
    "\n",
    "For the sake of having data to do deeper analysis I'm going to add in the comment_ranking data as a column eventually, but not calculate any summary stats off it for the API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9997338</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9997580</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9998036</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  ranking\n",
       "0  9997338        0\n",
       "1  9997580        0\n",
       "2  9998036        0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_ranking_df = pd.read_csv(\"data/hn_comment_ranking_query.csv\")\n",
    "comment_ranking_df = comment_ranking_df[['id','ranking']].copy()\n",
    "comment_ranking_df.set_index('id')\n",
    "comment_ranking_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in the missing ranking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.7 s, sys: 9.66 s, total: 1min 4s\n",
      "Wall time: 33.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds5 = ds4.drop(columns=['ranking'])\n",
    "ds5 = ds5.merge(comment_ranking_df, how='left', left_on='commentid', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This many nans: 7226283\n",
      "Index(['commentor', 'comment_time', 'commentid', 'parentid', 'comment_deleted',\n",
      "       'comment_dead', 'author', 'score', 'story_time', 'parent_type',\n",
      "       'parents_parent', 'parent_deleted', 'parent_dead', 'num_children',\n",
      "       'cleaned_comment', 'cleaned_title', 'comment_polarity',\n",
      "       'comment_subjectivity', 'id', 'ranking'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "nans = ds5.ranking.isna().sum()\n",
    "print('This many nans:', nans)\n",
    "print(ds5.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate commentors' sentiment statistics and make final dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns so API (JSON) is easier to read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.9 s, sys: 13.8 s, total: 51.6 s\n",
      "Wall time: 22.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds5 = ds5.rename(columns={'author': 'parent_author', 'cleaned_title': 'parent_title','score': 'parent_score', 'story_time': 'parent_time', 'ranking': 'comment_rank','commentid':'comment_id','parentid':'parent_id'})\n",
    "ds5 = ds5.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['commentor', 'comment_time', 'comment_id', 'parent_id',\n",
       "       'comment_deleted', 'comment_dead', 'parent_author', 'parent_score',\n",
       "       'parent_time', 'parent_type', 'parents_parent', 'parent_deleted',\n",
       "       'parent_dead', 'num_children', 'cleaned_comment', 'parent_title',\n",
       "       'comment_polarity', 'comment_subjectivity', 'comment_rank'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds5.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize comment subjectivity from sub `-1 to 1` to obj. Create booleans for +/- classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment helpers created...\n",
      "CPU times: user 24.5 s, sys: 1.93 s, total: 26.4 s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def SentimentHelpers(df):\n",
    "    df['comment_subjectivity'] = df['comment_subjectivity'].multiply(-1).add(.5).multiply(-2)\n",
    "    df['is_subjective'] = df['comment_subjectivity'].map(lambda x: True if (x < 0) else False)\n",
    "    df['is_negative'] = df['comment_polarity'].map(lambda x: True if (x < 0) else False)\n",
    "    print (\"Sentiment helpers created...\")\n",
    "\n",
    "\n",
    "%lprun -f SentimentHelpers z = SentimentHelpers(ds5)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Total time: 13.7587 s\n",
    "File: <ipython-input-29-425f0a3a30a2>\n",
    "Function: SentimentHelpers at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def SentimentHelpers(df):\n",
    "     2         1     183019.0 183019.0      1.3      df['comment_subjectivity'] = df['comment_subjectivity'].multiply(-2).add(1)\n",
    "     3         1    6802012.0 6802012.0     49.4      df['is_subjective'] = df['comment_subjectivity'].map(lambda x: True if (x < 0) else False)\n",
    "     4         1    6773480.0 6773480.0     49.2      df['is_negative'] = df['comment_polarity'].map(lambda x: True if (x < 0) else False)\n",
    "     5         1        159.0    159.0      0.0      print (\"Sentiment helpers created...\")\n",
    "\"\"\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15397309, 21)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_subjectivity</th>\n",
       "      <th>comment_rank</th>\n",
       "      <th>is_subjective</th>\n",
       "      <th>is_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.279167</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_subjectivity  comment_rank  is_subjective  is_negative\n",
       "0             -1.000000           NaN           True        False\n",
       "1              0.279167           1.0          False        False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ds5.shape)\n",
    "display(ds5.iloc[0:2, -4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create quadrant column for categorical class for use in Groupby function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 3s, sys: 10.7 s, total: 1min 14s\n",
      "Wall time: 58.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def DetermineQuadrant(df):\n",
    "    \"\"\"\n",
    "    Accepts DF\n",
    "    Creates Column\n",
    "    \"\"\"\n",
    "    df['polarity'] = df['comment_polarity'].map(lambda x: 'neg' if (x < 0) else 'pos')\n",
    "    df['basis'] = df['comment_subjectivity'].map(lambda x: 'sub' if (x < 0) else 'obj')\n",
    "    df = df.assign(quadrant=[str(x) + '_' + str(y) for x, y in zip(df['polarity'], df['basis'])])\n",
    "    df = df.drop(columns=['polarity','basis'])\n",
    "    return df\n",
    "\n",
    "%lprun -f DetermineQuadrant ds5 = DetermineQuadrant(ds5)\n",
    "\n",
    "\"\"\"\n",
    "Total time: 58.586 s\n",
    "File: <ipython-input-70-74685f1c6fc3>\n",
    "Function: DetermineQuadrant at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def DetermineQuadrant(df):\n",
    "     6         1    7027143.0 7027143.0     12.0      df['polarity'] = df['comment_polarity'].map(lambda x: 'neg' if (x < 0) else 'pos')\n",
    "     7         1    6990340.0 6990340.0     11.9      df['basis'] = df['comment_subjectivity'].map(lambda x: 'sub' if (x < 0) else 'obj')\n",
    "     8         1   27863275.0 27863275.0     47.6      df = df.assign(quadrant=[str(x) + '_' + str(y) for x, y in zip(df['polarity'], df['basis'])])\n",
    "     9         1   16705239.0 16705239.0     28.5      df = df.drop(columns=['polarity','basis'])\n",
    "    10         1         18.0     18.0      0.0      return df\n",
    "\"\"\"\n",
    "\n",
    "# This was helpful: https://stackoverflow.com/questions/11858472/string-concatenation-of-two-pandas-columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15397309, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_rank</th>\n",
       "      <th>is_subjective</th>\n",
       "      <th>is_negative</th>\n",
       "      <th>quadrant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>pos_sub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>pos_obj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_rank  is_subjective  is_negative quadrant\n",
       "0           NaN           True        False  pos_sub\n",
       "1           1.0          False        False  pos_obj"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ds5.shape)\n",
    "display(ds5.iloc[0:2, -4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send every row of these columns into a Json string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Uploaded\n",
      "CPU times: user 2min 6s, sys: 19.1 s, total: 2min 25s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def createCommentJSONrecords(df):\n",
    "    \"\"\"\n",
    "    Saves filtered dataframe columns as a json object oriented by row records. \n",
    "    Decodes the JSON string into a list containing 1 JSON object per row.\n",
    "    Adds new column in the dataframe that stores the row's JSON Object.\n",
    "    \"\"\"\n",
    "    saved = (df[['commentor','comment_time','comment_polarity','comment_subjectivity', \n",
    "                'is_subjective', 'is_negative','quadrant','parent_type','parent_author','parent_title',\n",
    "                'cleaned_comment','comment_rank','comment_id','parent_id']].to_json(orient='records'))\n",
    "    decoded = json.JSONDecoder().decode(saved)\n",
    "    df['comment_JSON'] = decoded\n",
    "    print( \"JSON Uploaded\")\n",
    "\n",
    "%lprun -f createCommentJSONrecords z = createCommentJSONrecords(ds5)\n",
    "\n",
    "\"\"\"\n",
    "CREATING JSON RECORDS\n",
    "Successful Run on df.shape = (15397309, 23)\n",
    "Total time: 134.944 s\n",
    "File: <ipython-input-23-68de41c26213>\n",
    "Function: createCommentJSONrecords at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def createCommentJSONrecords(df):                                              \n",
    "     7         1          3.0      3.0      0.0      saved = (df[['commentor','comment_time','comment_polarity','comment_subjectivity', \n",
    "     8         1          1.0      1.0      0.0                   'is_subjective', 'is_negative','parent_type', 'parent_author','parent_title',\n",
    "     9         1   51285889.0 51285889.0    38.0                  'cleaned_comment','ranking','commentid','parentid']].to_json(orient='records'))\n",
    "    10         1   82673003.0 82673003.0    61.3     decoded = json.JSONDecoder().decode(saved)\n",
    "    11         1     985481.0 985481.0      0.7      df['comment_JSON'] = decoded\n",
    "    12         1          2.0      2.0      0.0      return \"JSON Uploaded\"\n",
    "\n",
    "\"\"\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15397309, 23)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_subjective</th>\n",
       "      <th>is_negative</th>\n",
       "      <th>quadrant</th>\n",
       "      <th>comment_JSON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>pos_sub</td>\n",
       "      <td>{'commentor': 'Twisell', 'comment_time': 15489...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>pos_obj</td>\n",
       "      <td>{'commentor': 'camus2', 'comment_time': 139645...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_subjective  is_negative quadrant  \\\n",
       "0           True        False  pos_sub   \n",
       "1          False        False  pos_obj   \n",
       "\n",
       "                                        comment_JSON  \n",
       "0  {'commentor': 'Twisell', 'comment_time': 15489...  \n",
       "1  {'commentor': 'camus2', 'comment_time': 139645...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ds5.shape)\n",
    "display(ds5.iloc[0:2, -4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have 4 variables I want for plotting in JS:\n",
    "* 1. Polarity - Between -1 and 1. \n",
    "* 2. Subjectivity - Between -1 and 1. \n",
    "* 3. Time - UNIX time for the comment.\n",
    "* 4. Comment ID. \n",
    "\n",
    "I'll filter a DataFrame then use pandas.DataFrame.to_numpy()\n",
    "That will turn them into a numpy array. Like this:\n",
    "`array([[1. , 3. ], [2. , 4.5]])`\n",
    "Then I'll use `numpy.ndarray.tolist()` to change the array to a list of lists. \n",
    "Last, I'll create a new column and place my np array in it.\n",
    "That will make it so I can group them all later. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot Pairs Created\n",
      "CPU times: user 1min 12s, sys: 10.6 s, total: 1min 23s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def createPolaritySubjectivtyPairs(df):\n",
    "    \"\"\"\n",
    "    Pairs Polarity / Subjectivity points for plotting by row.\n",
    "    Pairs UNIX Epoch time / commentid points for plotting by row.\n",
    "    Combines both pairs into one list for each row.  \n",
    "    \"\"\"\n",
    "    df[\"polr_subj\"] = df[['comment_polarity','comment_subjectivity']].to_numpy().tolist()\n",
    "    df[\"time_id\"] = df[['comment_time','comment_id']].to_numpy().tolist()\n",
    "    df['polr_subj_time_id'] = df[['polr_subj','time_id']].to_numpy(dtype='object').tolist()\n",
    "    df = df.drop(columns=['polr_subj','time_id'])\n",
    "    print(\"Plot Pairs Created\")\n",
    "    return df\n",
    "\n",
    "%lprun -f createPolaritySubjectivtyPairs ds5 = createPolaritySubjectivtyPairs(ds5)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Total time: 78.0186 s\n",
    "File: <ipython-input-101-5f45d885fea7>\n",
    "Function: createPolaritySubjectivtyPairs at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def createPolaritySubjectivtyPairs(df):\n",
    "     7         1   12855352.0 12855352.0     16.5      df[\"polr_subj\"] = df[['comment_polarity','comment_subjectivity']].to_numpy().tolist()\n",
    "     8         1   17617648.0 17617648.0     22.6      df[\"time_id\"] = df[['comment_time','commentid']].to_numpy().tolist()\n",
    "     9         1   29883020.0 29883020.0     38.3      df['polr_subj_time_id'] = df[['polr_subj','time_id']].to_numpy(dtype='object').tolist()\n",
    "    10         1   17662446.0 17662446.0     22.6      df = df.drop(columns = [\"polr_subj\", \"time_id\"])\n",
    "    11         1        135.0    135.0      0.0      print(\"Plot Pairs Created\")\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15397309, 23)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_subjective</th>\n",
       "      <th>is_negative</th>\n",
       "      <th>quadrant</th>\n",
       "      <th>comment_JSON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>pos_sub</td>\n",
       "      <td>{'commentor': 'Twisell', 'comment_time': 15489...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>pos_obj</td>\n",
       "      <td>{'commentor': 'camus2', 'comment_time': 139645...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_subjective  is_negative quadrant  \\\n",
       "0           True        False  pos_sub   \n",
       "1          False        False  pos_obj   \n",
       "\n",
       "                                        comment_JSON  \n",
       "0  {'commentor': 'Twisell', 'comment_time': 15489...  \n",
       "1  {'commentor': 'camus2', 'comment_time': 139645...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ds5.shape)\n",
    "display(ds5.iloc[0:2, -4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO: \n",
    "\n",
    "* https://chrisalbon.com/python/data_wrangling/pandas_apply_operations_to_groups/\n",
    "* https://stackoverflow.com/questions/22219004/grouping-rows-in-list-in-pandas-groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- Commentors Summary Table ----#\n",
    "# Stats for Commenting\n",
    "#-- \n",
    "# Stats for Polarity\n",
    "# Stats for Subjectivity\n",
    "\n",
    "# Stats for Polarity (Grouped By Is_Polar?)\n",
    "# Stats for Subjectivity (Grouped By Is_Subjective?)\n",
    "# Stats for Polarity (Grouped by Quadrant)\n",
    "\n",
    "# List of top 10 Saltiest Comments\n",
    "# List of top 10 Happy Comments\n",
    "# Rank????? :) \n",
    "\n",
    "#----- Seperate Table ------ #\n",
    "# Top 1000 saltiest comments.\n",
    "# Top 1000 best comments.\n",
    "# Top 1000 Salty by Rank\n",
    "# Top 1000 Best by Rank\n",
    "\n",
    "\n",
    "# Mostest \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work in Progress: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity = ds5['comment_polarity'].groupby(ds5['commentor']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment_polarity'].groupby([df['commentor'], df['is_positive']]).describe().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_groups = ds5_test.groupby('commentor').groups\n",
    "\n",
    "pd.DataFrame(groups.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ds5.groupby('commentor').groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To group them \n",
    "def listEachPolaritySubjectivityPairs(df):\n",
    "    keys, values = df.sort_values('a').values.T\n",
    "    ukeys, index = np.unique(keys,True)\n",
    "    arrays = np.split(values,index[1:])\n",
    "    df2 = pd.DataFrame({'a':ukeys,'b':[list(a) for a in arrays]})\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polarity_stats(df):\n",
    "    commentor_table = (df['comment_polarity'].groupby(df['commentor'], as_index=False).describe()\n",
    "                       .rename({'25%': 'Q1','50%': 'Median','75%': 'Q3'}, axis='index')\n",
    "                       .add_prefix('b_all_'))\n",
    "    return commentor_table\n",
    "\n",
    "%lprun -f get_polarity_stats z = get_polarity_stats(ds5_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning to sort the hard way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Group by commentor to split into multiple tables\n",
    "df_smallA.groupby(\"commentor\").comment_polarity.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commentGetter(subdf): ### NOPE\n",
    "    # Sort the group\n",
    "    subdf = subdf.sort_values(by=['comment_polarity', 'comment_subjectivity'])\n",
    "    subdf['salty_comments_ids'] = subdf[[\"commentid\"]][0:10].to_json(orient='records')\n",
    "    subdf['sweet_comments_ids'] = subdf[[\"commentid\"]].tail(10).to_json(orient='records')\n",
    "    return subdf\n",
    "\n",
    "def simpleSort(): ### WINNNER\n",
    "    subdf = subdf.sort_values(by=['comment_polarity'])\n",
    "    return subdf\n",
    "\n",
    "def noSortGetter(subdf): ### ALMOST, STILL SLOW.\n",
    "    subdf['salty_comments_ids'] = subdf[[\"commentid\"]][0:10].to_json(orient='records')\n",
    "    subdf['sweet_comments_ids'] = subdf[[\"commentid\"]].tail(10).to_json(orient='records')\n",
    "    return subdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group (all) and Grab by commentor w/o Sort\n",
    "\n",
    "def noSortGetter(subdf): ### ALMOST, STILL SLOW. ~117 it/s\n",
    "    subdf['salty_comments_ids'] = subdf[[\"commentid\"]][0:10].to_json(orient='records')\n",
    "    subdf['sweet_comments_ids'] = subdf[[\"commentid\"]].tail(10).to_json(orient='records')\n",
    "    return subdf\n",
    "\n",
    "%lprun -f noSortGetter z = df_smallA[['commentor','comment_polarity','comment_subjectivity','commentid']].groupby(\"commentor\").progress_apply(noSortGetter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping presorted data, then applying a function to take the head or tail of it.\n",
    "```python\n",
    "Timer unit: 1e-06 s \n",
    "\n",
    "Total time: 276.015 s for 100,000 rows. Not very good. \n",
    "File: <ipython-input-76-c004a304a3b0>\n",
    "Function: noSortGetter at line 3\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     3                                           def noSortGetter(subdf): ### ALMOST, STILL SLOW.\n",
    "     4     35469  124971308.0   3523.4     45.3      subdf['salty_comments_ids'] = subdf[[\"commentid\"]][0:10].to_json(orient='records')\n",
    "     5     35469  150983242.0   4256.8     54.7      subdf['sweet_comments_ids'] = subdf[[\"commentid\"]].tail(10).to_json(orient='records')\n",
    "     6     35469      60223.0      1.7      0.0      return subdf\n",
    "```\n",
    "\n",
    "### Just remember though, I started with a for loop.... around ~10 it/s...\n",
    "\n",
    "```python\n",
    "# Create a list of the commentor's saltiest comments. \n",
    "outdf['salty_comments'] = subdf[['commentor','comment_time','comment_polarity',\n",
    "                        'ranking','cleaned_comment','cleaned_title',\n",
    "                        'comment_subjectivity']][0:9].to_json(orient='records')\n",
    "\n",
    "Timer unit: 1e-06 s \n",
    "\n",
    "Total time: 92.7974 s for 1000 rows.\n",
    "File: <ipython-input-89-b65c8bda2927>\n",
    "Function: loopSentimentAggegator at line 3\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     3                                           def loopSentimentAggegator(i):\n",
    "     4                                               # Select `subdf` for the selected commentor and sort\n",
    "     5      1000    1451170.0   1451.2      1.6      subdf = df_small_loop[df_small_loop['commentor'].values == i]\n",
    "     6      1000    3935143.0   3935.1      4.2      subdf = subdf.sort_values(by=['comment_polarity', 'comment_subjectivity'])\n",
    "     7                                           \n",
    "     8                                               # Initialize processing df `outdf`\n",
    "     9      1000       2812.0      2.8      0.0      commentor = i\n",
    "    10      1000    1259788.0   1259.8      1.4      outdf = pd.DataFrame([{ 'commentor': commentor }])\n",
    "    11                                               \n",
    "    12                                               # Comments: qty (int), first (unix time), last (unix time)\n",
    "    13      1000    1124149.0   1124.1      1.2      outdf[\"comments_qty\"] = len(subdf.index)\n",
    "    14      1000    1521732.0   1521.7      1.6      outdf[\"comments_first\"] = subdf.comment_time.max()\n",
    "    15      1000    1395105.0   1395.1      1.5      outdf[\"comments_last\"] = subdf.comment_time.min()\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The key is to Sort the data, then run the groupby operation.\n",
    "#### Lesson Learned here: Never try to sort in a loop if you can avoid it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleSort(subdf):\n",
    "    subdf = subdf.sort_values(by=['comment_polarity','comment_subjectivity'])\n",
    "    return subdf\n",
    "\n",
    "%lprun -f simpleSort z = simpleSort(ds5)\n",
    "\n",
    "\"\"\"\n",
    "Runs beautifully! \n",
    "Timer unit: 1e-06 s\n",
    "\n",
    "Total time: 46.4984 s\n",
    "File: <ipython-input-114-b21908e70c09>\n",
    "Function: simpleSort at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def simpleSort(subdf):\n",
    "     2         1   46498373.0 46498373.0    100.0      subdf = subdf.sort_values(by=['comment_polarity','comment_subjectivity'])\n",
    "     3         1          2.0      2.0      0.0      return subdf\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First run\n",
    "```python\n",
    "%lprun output - simple sort 500,000 rows. \n",
    "Timer unit: 1e-06 s\n",
    "\n",
    "Total time: 0.031263 s\n",
    "File: <ipython-input-64-3e739ef6d38a>\n",
    "Function: simpleSort at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def simpleSort(subdf):\n",
    "     2         1      31261.0  31261.0    100.0      subdf = subdf.sort_values(by=['comment_polarity','comment_subjectivity'])\n",
    "     3         1          2.0      2.0      0.0      return subdf\n",
    "     \n",
    "```\n",
    "\n",
    "Second run\n",
    "```python\n",
    "Second run lprun output - simple sort. 1,000,000 rows.\n",
    "Timer unit: 1e-06 s\n",
    "\n",
    "Total time: 0.469898 s\n",
    "File: <ipython-input-69-3e739ef6d38a>\n",
    "Function: simpleSort at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def simpleSort(subdf):\n",
    "     2         1     469896.0 469896.0    100.0      subdf = subdf.sort_values(by=['comment_polarity','comment_subjectivity'])\n",
    "     3         1          2.0      2.0      0.0      return subdf\n",
    "\n",
    "```\n",
    "\n",
    "Final sorting run was last.\n",
    "``` python\n",
    "Final Run - Less than 1 second. 15,397,309 rows\n",
    "Timer unit: 1e-06 s\n",
    "\n",
    "Total time: 0.470333 s\n",
    "File: <ipython-input-73-3e739ef6d38a>\n",
    "Function: simpleSort at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def simpleSort(subdf):\n",
    "     2         1     470331.0 470331.0    100.0      subdf = subdf.sort_values(by=['comment_polarity','comment_subjectivity'])\n",
    "     3         1          2.0      2.0      0.0      return subdf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby(df.L)\n",
    "grouped_df.groups.series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series({x : y.b.tolist() for x , y in df.groupby('a')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smallA.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabTen(subdf):\n",
    "    subdf['comment_ids'] = subdf.groupby('commentor')['commentid'].head(10).to_json()\n",
    "    return subdf\n",
    "\n",
    "%lprun -f grabTen z = grabTen(df_smallC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabAllPoints(subdf):\n",
    "    # collect upto 4000 comment data points for plotting?\n",
    "    subdf[\"polsub_points\"] = subdf.groupby('commentor')[['comment_polarity','comment_subjectivity']].head(10000).to_json()\n",
    "    return subdf                                            \n",
    "\n",
    "%lprun -f grabAllPoints z = grabAllPoints(df_smallC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabAllPoints(subdf):\n",
    "    # collect upto 4000 comment data points for plotting?\n",
    "    subdf[\"polsub_points\"] = subdf.groupby('commentor')['comment_polarity','comment_subjectivity'].apply(lambda x)\n",
    "    return subdf                                            \n",
    "\n",
    "%lprun -f grabAllPoints z = grabAllPoints(df_smallC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_coord (df):\n",
    "    df[\"polsub_points\"] = df.to_json()\n",
    "    return df\n",
    "\n",
    "def grabAllPoints(df):\n",
    "    df['polarity_points'] = df.groupby('commentor').comment_polarity.apply(pd.Series.tolist)\n",
    "    return df\n",
    "\n",
    "%lprun -f grabAllPoints y = grabAllPoints(df_smallC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small_loop = ds5[0:10000]\n",
    "\n",
    "def loopSentimentAggegator(i):\n",
    "    # Select `subdf` for the selected commentor and sort\n",
    "    subdf = df_small_loop[df_small_loop['commentor'].values == i]\n",
    "    subdf = subdf.sort_values(by=['comment_polarity', 'comment_subjectivity'])\n",
    "\n",
    "    # Initialize processing df `outdf`\n",
    "    commentor = i\n",
    "    outdf = pd.DataFrame([{ 'commentor': commentor }])\n",
    "    \n",
    "    # Comments: qty (int), first (unix time), last (unix time)\n",
    "    outdf[\"comments_qty\"] = len(subdf.index)\n",
    "    outdf[\"comments_first\"] = subdf.comment_time.max()\n",
    "    outdf[\"comments_last\"] = subdf.comment_time.min()\n",
    "\n",
    "    # Filters\n",
    "    p_pos = (subdf['comment_polarity'] >= 0.0)\n",
    "    p_neg = (subdf['comment_polarity'] < 0.0)\n",
    "    b_pos = (subdf['comment_subjectivity'] >= 0.0)\n",
    "    b_neg = (subdf['comment_subjectivity'] < 0.0)\n",
    "    \n",
    "    q1_filter = (p_pos & b_pos)\n",
    "    q2_filter = (p_pos & b_neg)\n",
    "    q3_filter = (p_neg & b_pos)\n",
    "    q4_filter = (p_neg & b_neg)\n",
    "                                  \n",
    "    \n",
    "    \"\"\"\n",
    "    ####################################################################\n",
    "    POLARITY: \"Emotional sentiment\"\n",
    "    Is the sentiment of the written piece positive or negative?\n",
    "    How many negative? Mean of negative? Total negativity?\n",
    "    How many positive? Mean of positive? Total positivity?\n",
    "    \"\"\"\n",
    "    # (ALL POLARITY): \n",
    "    outdf = outdf.join(subdf.comment_polarity.describe().rename({'25%': 'Q1','50%': 'Median','75%': 'Q3'}, axis='index')\n",
    "                      .loc[['mean','std','min','max','median','Q1','Q3']]\n",
    "                      .add_prefix('p_all_').to_frame().T.reset_index(drop=True), how='left')\n",
    "    outdf[\"p_all_sum\"] = subdf.comment_polarity.sum()\n",
    "    \n",
    "    # (NEGATIVE) \n",
    "    outdf[\"p_neg_count\"] = subdf[p_neg].comment_polarity.count()\n",
    "    outdf[\"p_neg_mean\"] = subdf[p_neg].comment_polarity.mean()\n",
    "    outdf[\"p_neg_sum\"] = subdf[p_neg].comment_polarity.sum() \n",
    "    \n",
    "    # (POSITIVE) \n",
    "    outdf[\"p_pos_count\"] = subdf[p_pos].comment_polarity.count()\n",
    "    outdf[\"p_pos_mean\"] = subdf[p_pos].comment_polarity.mean()\n",
    "    outdf[\"p_pos_sum\"] = subdf[p_pos].comment_polarity.sum() \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    ####################################################################\n",
    "    BASIS: \"Subjectivity\" \n",
    "    Is the written perspective's basis subjective or objective?\n",
    "    For SUBJECTIVE or OBJECTIVE, how many are there?\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    # (ALL BASIS)\n",
    "    outdf = outdf.join(subdf.comment_subjectivity.describe().rename({'25%': 'Q1','50%': 'Median','75%': 'Q3'}, axis='index')\n",
    "                      .loc[['mean','std','min','max','median','Q1','Q3']]\n",
    "                      .add_prefix('b_all_').to_frame().T.reset_index(drop=True),how='left')\n",
    "    outdf[\"b_all_sum\"] = subdf.comment_subjectivity.sum()\n",
    "    \n",
    "    # (SUBJECTIVE) (Negative)\n",
    "    #---->Total Salty Comments<------\n",
    "    outdf[\"b_neg_count\"] = subdf[b_neg].comment_subjectivity.count()\n",
    "    outdf[\"b_neg_mean\"] = subdf[b_neg].comment_subjectivity.mean()\n",
    "    outdf[\"b_neg_sum\"] = subdf[b_neg].comment_subjectivity.sum()\n",
    "    \n",
    "    # (OBJECTIVE) (Positive)\n",
    "    outdf[\"b_pos_count\"] = subdf[b_pos].comment_subjectivity.count()\n",
    "    outdf[\"b_pos_mean\"] = subdf[b_pos].comment_subjectivity.mean()\n",
    "    outdf[\"b_pos_sum\"] = subdf[b_pos].comment_subjectivity.sum()\n",
    "    \n",
    "    \"\"\"\n",
    "    ####################################################################\n",
    "    BASIS VS POLARITY: CrossTab. \n",
    "    Are certain parings of Polarity & Basis more common? \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    p_pos = (subdf['comment_polarity'] >= 0.0)\n",
    "    p_neg = (subdf['comment_polarity'] < 0.0)\n",
    "    b_pos = (subdf['comment_subjectivity'] >= 0.0)\n",
    "    b_neg = (subdf['comment_subjectivity'] < 0.0)\n",
    "                                  \n",
    "\n",
    "    #### 24 features\n",
    "    # Q1: POLARITY(+)/BASIS(+) - 'Positive and Objective'  (The Builder)\n",
    "    outdf[\"Q1_pp_p_count\"] = subdf[q2_filter].comment_polarity.count()\n",
    "    outdf[\"Q1_pp_p_mean\"] = subdf[q2_filter].comment_polarity.mean()\n",
    "    outdf[\"Q1_pp_p_sum\"] = subdf[q2_filter].comment_polarity.sum() \n",
    "    \n",
    "    outdf[\"Q1_pp_b_count\"] = subdf[q2_filter].comment_subjectivity.count()\n",
    "    outdf[\"Q1_pp_b_mean\"] = subdf[q2_filter].comment_subjectivity.mean()\n",
    "    outdf[\"Q1_pp_b_sum\"] = subdf[q2_filter].comment_subjectivity.sum()\n",
    "    \n",
    "    \n",
    "    # Q2: POLARITY(+)/BASIS(-) - 'Positive and Subjective' (The Feeler)\n",
    "    outdf[\"Q2_pn_p_count\"] = subdf[q2_filter].comment_polarity.count()\n",
    "    outdf[\"Q2_pn_p_mean\"] = subdf[q2_filter].comment_polarity.mean()\n",
    "    outdf[\"Q2_pn_p_sum\"] = subdf[q2_filter].comment_polarity.sum() \n",
    "    \n",
    "    outdf[\"Q2_pn_p_count\"] = subdf[q2_filter].comment_subjectivity.count()\n",
    "    outdf[\"Q2_pn_p_mean\"] = subdf[q2_filter].comment_subjectivity.mean()\n",
    "    outdf[\"Q2_pn_p_sum\"] = subdf[q2_filter].comment_subjectivity.sum()\n",
    "    \n",
    "    \n",
    "    # Q3: POLARITY(-)/BASIS(+) - 'Negative and Objective'  (The Critic)\n",
    "    outdf[\"Q3_np_p_count\"] = subdf[q3_filter].comment_polarity.count()\n",
    "    outdf[\"Q3_np_p_mean\"] = subdf[q3_filter].comment_polarity.mean()\n",
    "    outdf[\"Q3_np_p_sum\"] = subdf[q3_filter].comment_polarity.sum() \n",
    "    \n",
    "    outdf[\"Q3_np_b_count\"] = subdf[q3_filter].comment_subjectivity.count()\n",
    "    outdf[\"Q3_np_b_mean\"] = subdf[q3_filter].comment_subjectivity.mean()\n",
    "    outdf[\"Q3_np_b_sum\"] = subdf[q3_filter].comment_subjectivity.sum()\n",
    "    \n",
    "    \n",
    "    # Q4: POLARITY(-)/BASIS(-) - 'Negative and Subjective' (The Salty)\n",
    "    outdf[\"Q4_nn_p_count\"] = subdf[q4_filter].comment_polarity.count()\n",
    "    outdf[\"Q4_nn_p_mean\"] = subdf[q4_filter].comment_polarity.mean()\n",
    "    outdf[\"Q4_nn_p_sum\"] = subdf[q4_filter].comment_polarity.sum() \n",
    "    \n",
    "    outdf[\"Q4_nn_b_count\"] = subdf[q4_filter].comment_subjectivity.count()\n",
    "    outdf[\"Q4_nn_b_mean\"] = subdf[q4_filter].comment_subjectivity.mean()\n",
    "    outdf[\"Q4_nn_b_sum\"] = subdf[q4_filter].comment_subjectivity.sum()\n",
    "\"\"\";\n",
    "\n",
    "    # Upvote Ranking Metrics -  Mean & Count & Sum\n",
    "    outdf = outdf.join(subdf.ranking.describe()\n",
    "                      .loc[['count','max','mean']]\n",
    "                      .add_prefix('upvotes_')\n",
    "                      .to_frame().T.reset_index(drop=True),how='left')      \n",
    "    outdf['upvotes_sum'] = subdf.ranking.sum()\n",
    "\n",
    "    #GROUPED_OBJECTS#################################################################### \n",
    "    # All of Commentor's Polarity / Subjectivity points in list for plotting.\n",
    "    outdf[\"polsub_points\"] = subdf[['comment_polarity','comment_subjectivity']].to_json()  \n",
    "    \n",
    "    # Create the second output, a list of the commentor's saltiest comments. \n",
    "    outdf['salty_comments'] = subdf[['commentor','comment_time','comment_polarity',\n",
    "                            'ranking','cleaned_comment','parent_title',\n",
    "                            'comment_subjectivity']][0:9].to_json(orient='records')\n",
    "    \n",
    "    # Ten most positive comments\n",
    "    #    outdf['sweet_comments'] = subdf[['commentor','comment_time','comment_polarity',\n",
    "    #                            'ranking','cleaned_comment','parent_title',\n",
    "    #                            'comment_subjectivity']].tail(10).to_json(orient='records')\n",
    "    \n",
    "\n",
    "    \n",
    "    outputDF = outdf\n",
    "    return outputDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test function with single commentor to ensure output is good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "commentorList = ds5.commentor.unique().tolist()\n",
    "print(\"There are this many unique commentors:\", len(commentorList))\n",
    "c_list = pd.DataFrame(commentorList)\n",
    "c_list.columns = ['commentor']\n",
    "display(c_list.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f loopSentimentAggegator testingDF = loopSentimentAggegator('eli')\n",
    "display(testingDF.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dont use homebrewed aggregation functions by commentor to entire dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = []\n",
    "\n",
    "%lprun -f loopSentimentAggegator c_list[0:1000].commentor.progress_apply(lambda x: results1.append(loopSentimentAggegator(x)))\n",
    "\n",
    "final_testing= pd.concat(results1)\n",
    "final_testing.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "d.groupby(\n",
    " 'x'\n",
    " ).head(\n",
    " K\n",
    " ).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate aggregation outputs (list of dfs) into a single final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTableResults = pd.concat(results)\n",
    "print(finalTableResults.shape)\n",
    "display(finalTableResults.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save final results to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTableResults.to_csv('data/hn_commentor_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save final results to AVRO (just to be safe) :). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdx.to_avro('data/hn_commentor_data.avro', finalTableResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check lengths & tail to make sure it looks right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvsaved = df.read_csv('data/hn_commentor_data.csv')\n",
    "avrosaved = pdx.read_avro('data/hn_commentor_data.avro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Saved csv shape:', csvsaved.shape)\n",
    "print('Saved avro shape:', avrosaved.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(avrosaved.tail(3))\n",
    "display(csvsaved.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUCCESS!  Now just need to get it into a AWS RDS PostgreSQL instance. : )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I feel like this should have worked... but it didn't. Any insights as to why it failed? \n",
    "``` python\n",
    "from dask.distributed import Client, LocalCluster\n",
    "client = Client()  # This is actually the following two commands\n",
    "cluster = LocalCluster()\n",
    "\n",
    "%%time\n",
    "# Trying this with dask\n",
    "ds = dd.from_pandas(ds2.cleaned_comment, npartitions=1000)\n",
    "res = ds.apply(lambda x: get_sentiment(x), meta={'z':'str'})\n",
    "res.compute(scheduler='threads', num_workers=8)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Material: The Graveyard - Ideas that didn't work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This didn't work because df.to_sql() is sloooooooow. Just send the CSV straight to PostgreSQL. \n",
    "\n",
    "``` python\n",
    "def verify_output(pgres_engine, table_name):\n",
    "    # ______  verify output-table contents ____\n",
    "    query = 'SELECT * FROM ' + table_name + ' LIMIT 10;'\n",
    "    for row in pgres_engine.execute(query).fetchall():\n",
    "        print(row)\n",
    "    return\n",
    "\n",
    "def run_conversion(pgres_engine):\n",
    "    # ___ process tables ____\n",
    "    df = pdx.read_avro('data/hn_commentors_db.avro')\n",
    "    schema_name = 'lambdaRPG'\n",
    "    tables = ['commentor_data']\n",
    "    df.to_sql(table_name,\n",
    "              if_exists='replace',\n",
    "              con=pgres_engine,\n",
    "              schema=schema_name,\n",
    "              chunksize=10)\n",
    "    verify_output(pgres_engine, table_name)\n",
    "    return\n",
    "\n",
    "def runARVOtoSQL():\n",
    "    # __ Connect to postgres (SQLalchemy.engine) ____\n",
    "    dbname = ''\n",
    "    user = ''\n",
    "    host = ''\n",
    "    password = ''\n",
    "    file = open('aws.pwd', 'r')\n",
    "    ctr = 1\n",
    "    for line in file:\n",
    "        line = line.replace('\\n', '')\n",
    "        if ctr == 1: dbname = line\n",
    "        if ctr == 2: user = line\n",
    "        if ctr == 3: host = line\n",
    "        if ctr == 4: passw = line\n",
    "        ctr = ctr + 1\n",
    "    pgres_str = 'postgresql+psycopg2://'+user+':'+passw+'@'+host+'/'+dbname\n",
    "    pgres_engine = create_engine(pgres_str)\n",
    "    run_conversion(pgres_engine)\n",
    "    print('Conversion successful.....')\n",
    "    return\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "* This didn't work because I need to learn more dask. \n",
    "``` python\n",
    "ds3['comment_sentiment_dask'] = ds3['cleaned_comment'].apply(lambda x: get_sentiment(x)).compute(scheduler='threads')```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This also didn't work. Same reason.\n",
    "``` python\n",
    "dsr3 = ds2\n",
    "dsr3['cleaned_comment'] = dsr3.text.apply(lambda x: noURLS(noHTML(encode_decode(x)))).compute()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This dask layout worked for a few parts but didn't want to thread. And it kept failing because of a deep error. \n",
    "\n",
    "``` python\n",
    "dsr3 = dd.from_pandas(ds2, npartitions=2000)\n",
    "\n",
    "finalDF = dsr2\n",
    "def fin (daskDataframe):\n",
    "    daskDataframe['comment_sentiment'] = daskDataframe.text.apply(lambda x: get_sentiment(noURLS(noHTML(encode_decode(x)))))\n",
    "    daskDataframe['cleaned_comment'] = daskDataframe.text.apply(lambda x: noURLS(noHTML(encode_decode(x))))\n",
    "    return finalDF\n",
    "\n",
    "with ProgressBar():\n",
    "    res = fin(dsr2).compute()``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is helpful. \n",
    "\n",
    "``` console\n",
    "Where to find the dask distributed Bokeh dashboard on aws. \n",
    "\n",
    "URL of accessing Dask Dashboard will be:\n",
    "https://myinstance.notebook.us-east-1.sagemaker.aws/proxy/8787/```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks for reading! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOTS OF ROOM FOR IMPROVEMENT I GUESS.\n",
    "\n",
    "#len(commentorList)+1\n",
    "results = []\n",
    "for j in tqdm(commentorList[0:100]):\n",
    "    newDF = loopSentimentAggegator(j)\n",
    "    results.append(newDF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
